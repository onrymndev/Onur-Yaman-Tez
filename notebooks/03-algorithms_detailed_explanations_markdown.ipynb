{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ecad61ae-9d6d-4f17-bed2-269e249847d9",
   "metadata": {},
   "source": [
    "# üîÑ 1. SVM-SMOTE (Support Vector Machine ‚Äì Synthetic Minority Oversampling Technique)\n",
    "\n",
    "## **Introduction**\n",
    "\n",
    "SVM-SMOTE is an advanced version of SMOTE that uses a Support Vector Machine (SVM) to detect the decision boundary. Synthetic samples are then generated **only around the minority class instances that lie near the boundary**, helping the classifier to focus on difficult cases.\n",
    "\n",
    "---\n",
    "\n",
    "## **Algorithm Steps**\n",
    "\n",
    "1. Train an SVM classifier on the dataset.\n",
    "2. Identify the **support vectors** of the **minority class**.\n",
    "3. For each support vector:\n",
    "\n",
    "   - Find its **k-nearest neighbors** among the minority class.\n",
    "   - Generate synthetic samples as:\n",
    "\n",
    "     $$\n",
    "     x_{\\text{new}} = x_i + \\lambda \\cdot (x_{nn} - x_i)\n",
    "     $$\n",
    "\n",
    "     where:\n",
    "\n",
    "     $$\n",
    "     x_i = \\text{support vector from the minority class}\n",
    "     $$\n",
    "\n",
    "     $$\n",
    "     x_{nn} = \\text{k-th nearest neighbor of } x_i\n",
    "     $$\n",
    "\n",
    "     $$\n",
    "     \\lambda = \\text{a random value drawn from the uniform distribution between 0 and 1}\n",
    "     $$\n",
    "\n",
    "4. Repeat the process until the desired number of synthetic samples is generated.\n",
    "\n",
    "---\n",
    "\n",
    "## **Mathematical Formulation**\n",
    "\n",
    "$$\n",
    "x_{\\text{new}} = x_i + \\lambda \\cdot (x_{nn} - x_i)\n",
    "$$\n",
    "\n",
    "Where:\n",
    "\n",
    "$$\n",
    "x_i = \\text{support vector from the minority class}\n",
    "$$\n",
    "\n",
    "$$\n",
    "x_{nn} = \\text{k-th nearest neighbor of } x_i \\text{ from the minority class}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\lambda = \\text{random value between 0 and 1}\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "## **Advantages**\n",
    "\n",
    "- Targets borderline samples, which are critical for classification.\n",
    "- Helps improve performance on imbalanced datasets.\n",
    "- Avoids oversampling safe or redundant regions.\n",
    "\n",
    "---\n",
    "\n",
    "## **Limitations**\n",
    "\n",
    "- Requires SVM training, which can be computationally expensive.\n",
    "- Sensitive to SVM hyperparameters (e.g., kernel, margin).\n",
    "- Might not perform well if the SVM does not find a reliable decision boundary.\n",
    "\n",
    "---\n",
    "\n",
    "# üîÅ 2. SMOTE-ENN (SMOTE + Edited Nearest Neighbors)\n",
    "\n",
    "## **Introduction**\n",
    "\n",
    "SMOTE-ENN is a **hybrid technique** that combines:\n",
    "\n",
    "- **SMOTE** to oversample the minority class with synthetic samples.\n",
    "- **ENN (Edited Nearest Neighbors)** to remove noisy or misclassified samples based on nearest neighbor disagreement.\n",
    "\n",
    "The combination leads to a **balanced and cleaner dataset**, improving classification performance.\n",
    "\n",
    "---\n",
    "\n",
    "## **Algorithm Steps**\n",
    "\n",
    "1. Generate synthetic samples using SMOTE:\n",
    "\n",
    "   $$\n",
    "   x_{\\text{new}} = x_i + \\lambda \\cdot (x_{nn} - x_i)\n",
    "   $$\n",
    "\n",
    "   Where:\n",
    "\n",
    "   $$\n",
    "   x_i = \\text{a minority class instance}\n",
    "   $$\n",
    "\n",
    "   $$\n",
    "   x_{nn} = \\text{k-th nearest neighbor of } x_i \\text{ from the minority class}\n",
    "   $$\n",
    "\n",
    "   $$\n",
    "   \\lambda = \\text{random value between 0 and 1}\n",
    "   $$\n",
    "\n",
    "2. Apply ENN:\n",
    "   - For each sample:\n",
    "\n",
    "     $$\n",
    "     x = \\text{a data point in the dataset}\n",
    "     $$\n",
    "\n",
    "   - Find the **k-nearest neighbors** of \\( x \\).\n",
    "   - If the **majority of neighbors have a different class label**, remove \\( x \\).\n",
    "\n",
    "---\n",
    "\n",
    "## **Mathematical Formulation**\n",
    "\n",
    "**Step 1 ‚Äì SMOTE:**\n",
    "\n",
    "$$\n",
    "x_{\\text{new}} = x_i + \\lambda \\cdot (x_{nn} - x_i)\n",
    "$$\n",
    "\n",
    "Where:\n",
    "\n",
    "$$\n",
    "x_i = \\text{minority class instance}\n",
    "$$\n",
    "\n",
    "$$\n",
    "x_{nn} = \\text{k-th nearest neighbor of } x_i \\text{ from the minority class}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\lambda = \\text{random value between 0 and 1}\n",
    "$$\n",
    "\n",
    "**Step 2 ‚Äì ENN:**\n",
    "\n",
    "$$\n",
    "x = \\text{a sample in the dataset}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\text{If the majority of the } k \\text{ nearest neighbors of } x \\text{ have a different class label, remove } x\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "## **Advantages**\n",
    "\n",
    "- Balances the dataset while reducing noise.\n",
    "- Eliminates mislabeled or borderline instances that confuse the classifier.\n",
    "- Improves model performance by simplifying the class boundaries.\n",
    "\n",
    "---\n",
    "\n",
    "## **Limitations**\n",
    "\n",
    "- Can remove valuable borderline points.\n",
    "- Computationally more expensive than standalone SMOTE.\n",
    "- Needs careful tuning of \\( k \\) in both SMOTE and ENN.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42a8ed73-dfd5-42c8-be4a-3457a4d42a6e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "14b9884b-6f5e-42c8-86d2-a799f7d5e0f8",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "## üîÑ 3. ENN (Edited Nearest Neighbors) with Distance-Weighted Voting\n",
    "\n",
    "- After oversampling, the dataset may contain **noisy or overlapping instances**. The ENN step removes samples whose class label **disagrees with the majority of their neighbors**.\n",
    "- Instead of using simple majority voting, this approach introduces **distance-weighted voting** ‚Äî making the contribution of each neighbor dependent on its proximity to the point.\n",
    "\n",
    "---\n",
    "\n",
    "##  Weighting Strategies for ENN\n",
    "\n",
    "This version of ENN supports multiple **distance-based weighting mechanisms**, allowing for more nuanced decisions during noise removal.\n",
    "\n",
    "| **Strategy**         | **Mathematical Formula** | **Description** |\n",
    "|----------------------|--------------------------|------------------|\n",
    "| **Inverse Distance** | $$ w = \\frac{1}{d + \\varepsilon} $$ | Closer neighbors get more weight. Avoids division by zero with \\( \\varepsilon \\). |\n",
    "| **Gaussian**         | $$ w = \\exp\\left( -\\frac{d^2}{2\\sigma^2} \\right) $$ | Weights decrease smoothly with squared distance. \\( \\sigma \\) controls the spread. |\n",
    "| **Exponential**      | $$ w = \\exp(-\\alpha d) $$ | Exponentially decreases as distance increases. \\( \\alpha \\) controls the rate. |\n",
    "| **Rank-based**       | $$ w = \\frac{1}{\\text{rank} + 1} $$ | Uses neighbor rank instead of distance. First neighbor gets highest weight. |\n",
    "| **Adaptive Power**   | $$ w = \\frac{1}{d^\\beta + \\varepsilon} $$ | Flexible decay based on \\( \\beta \\); allows tuning for sharper or smoother weighting. |\n",
    "\n",
    "---\n",
    "\n",
    "##  Decision Rule (ENN Cleaning)\n",
    "\n",
    "For each sample in the dataset:\n",
    "1. Find its \\( k \\)-nearest neighbors.\n",
    "2. Assign weights to neighbor labels using one of the selected strategies.\n",
    "3. Predict the sample's class label via **weighted majority voting**.\n",
    "4. **Remove the sample** from the dataset if its true label **does not match** the predicted label.\n",
    "\n",
    "This process effectively removes:\n",
    "- Ambiguous samples near the decision boundary,\n",
    "- Potential label noise,\n",
    "- Overlapping samples caused by SMOTE.\n",
    "\n",
    "---\n",
    "\n",
    "## Benefits of This Approach\n",
    "\n",
    "-  More **robust noise removal** compared to standard ENN.\n",
    "-  Customizable weighting provides **flexibility** for different datasets.\n",
    "-  Reduces the risk of **overfitting** on synthetic or noisy examples.\n",
    "\n",
    "---\n",
    "\n",
    "## Considerations\n",
    "\n",
    "-  Requires **tuning** of additional hyperparameters (e.g., `sigma`, `alpha`, `beta`) for best results.\n",
    "-  Computationally more **intensive** due to extra neighbor weighting.\n",
    "-  Risk of **over-cleaning** the dataset if ENN is too aggressive.\n",
    "\n",
    "---\n",
    "\n",
    "## Summary\n",
    "\n",
    "This SMOTEENN variant blends **synthetic oversampling** with **intelligent undersampling**. By integrating **distance-aware voting**, it provides a smarter way to remove misleading data points ‚Äî offering a powerful and flexible tool for tackling **class imbalance problems** in real-world datasets.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a07df76-ec15-4b96-b224-d4baa163c48d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1ebaf060-f69f-47b6-b7ea-32bfde56a2c2",
   "metadata": {},
   "source": [
    "# üîÑ 4. SMOTIFIED-GAN FOR CLASS IMBALANCE PROBLEM\n",
    "\n",
    "## **Introduction**\n",
    "\n",
    "SMOTified-GAN is a **combination** of **SMOTE** (Synthetic Minority Oversampling Technique) and **GAN** (Generative Adversarial Networks) that aims to overcome the deficiencies of both methods. The process starts by using **SMOTE** to generate synthetic minority class samples and then refines these samples using **GAN**. This results in **more diverse** and **realistic** minority class samples.\n",
    "\n",
    "---\n",
    "\n",
    "## **Mathematical Formulation**\n",
    "\n",
    "### **SMOTE Step**\n",
    "\n",
    "SMOTE generates synthetic samples by **interpolating** between nearest neighbors of the minority class samples. Given a minority class data point $x_i$ and its **k-nearest neighbors** $\\{x_1, x_2, \\dots, x_k\\}$, the synthetic sample is generated by interpolation:\n",
    "\n",
    "$$\n",
    "x_{\\text{synthetic}} = x_i + \\lambda (x_j - x_i), \\quad j \\in \\{1, 2, \\dots, k\\}, \\quad \\lambda \\sim \\mathcal{U}(0, 1)\n",
    "$$\n",
    "\n",
    "Where:\n",
    "- $x_i$ = an original sample from the minority class,\n",
    "- $x_j$ = one of the k-nearest neighbors of $x_i$,\n",
    "- $\\lambda$ = random scalar chosen from a uniform distribution.\n",
    "\n",
    "### **GAN Step**\n",
    "\n",
    "The **GAN** consists of two networks: the **Generator ($G$)** and the **Discriminator ($D$)**. The goal of the **Generator** is to produce synthetic data $x_{\\text{fake}}$ that cannot be distinguished from real data by the **Discriminator**.\n",
    "\n",
    "- **Generator Output:**\n",
    "\n",
    "$$\n",
    "x_{\\text{fake}} = G(z), \\quad z \\sim \\mathcal{U}(0, 1)\n",
    "$$\n",
    "\n",
    "Where:\n",
    "- $z$ = random noise vector,\n",
    "- $x_{\\text{fake}}$ = synthetic data sample generated by the Generator.\n",
    "\n",
    "- **Discriminator Objective:**\n",
    "\n",
    "$$\n",
    "\\mathcal{L}_D = \\frac{1}{2} \\left[ \\mathbb{E}_{x \\sim P_{\\text{real}}} \\left[ (D(x) - 1)^2 \\right] + \\mathbb{E}_{z \\sim P_z} \\left[ (D(G(z)))^2 \\right] \\right]\n",
    "$$\n",
    "\n",
    "Where:\n",
    "- $D(x)$ = discriminator prediction for real data $x$,\n",
    "- $D(G(z))$ = discriminator prediction for generated data $x_{\\text{fake}}$.\n",
    "\n",
    "- **Generator Objective:**\n",
    "\n",
    "$$\n",
    "\\mathcal{L}_G = \\mathbb{E}_{z \\sim P_z} \\left[ (D(G(z)) - 1)^2 \\right]\n",
    "$$\n",
    "\n",
    "The Generator aims to minimize the loss function to fool the Discriminator into classifying generated data as real.\n",
    "\n",
    "---\n",
    "\n",
    "### **SMOTified-GAN Formulation**\n",
    "\n",
    "**SMOTified-GAN** enhances the GAN architecture by replacing the random noise $z$ in the generator with the synthetic samples generated by **SMOTE**. This allows the GAN to start with **realistic synthetic samples** instead of random noise, improving the quality of generated samples.\n",
    "\n",
    "1. **Discriminator Score:**\n",
    "\n",
    "$$\n",
    "\\mathcal{L}_D = \\mathbb{E}_{x^* \\sim P_{\\text{real}}} \\left[ \\log D(x^*) \\right] + \\mathbb{E}_{u \\sim P_{\\text{SMOTE}}} \\left[ \\log(1 - D(G(u))) \\right]\n",
    "$$\n",
    "\n",
    "Where:\n",
    "- $x^*$ = real minority class samples,\n",
    "- $u$ = oversampled synthetic minority class data from **SMOTE**.\n",
    "\n",
    "2. **Generator Score:**\n",
    "\n",
    "$$\n",
    "\\mathcal{L}_G = - \\mathbb{E}_{u \\sim P_{\\text{SMOTE}}} \\left[ \\log D(G(u)) \\right]\n",
    "$$\n",
    "\n",
    "Where:\n",
    "- $u$ = over-sampled data from **SMOTE** used as input to the **Generator**,\n",
    "- $G(u)$ = synthetic data generated by the Generator.\n",
    "\n",
    "---\n",
    "\n",
    "## **Summary**\n",
    "\n",
    "**SMOTified-GAN** is an advanced technique for addressing **class imbalance** by combining the strengths of **SMOTE** and **GAN**:\n",
    "- **SMOTE** generates initial synthetic minority samples using interpolation.\n",
    "- **GAN** refines these samples, making them more diverse and realistic by training the generator to fool the discriminator.\n",
    "\n",
    "This synergy results in high-quality synthetic samples that can be used to balance datasets and improve model performance.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44e9b9d1-cbcf-48b3-a85e-555093762a25",
   "metadata": {},
   "outputs": [],
   "source": [
    "https://medium.com/totalenergies-digital-factory/imbalanced-data-ml-smote-and-its-variants-c69a4b32f7e7"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
