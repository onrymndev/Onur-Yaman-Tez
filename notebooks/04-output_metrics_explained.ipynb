{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "826659a2-5541-4e9c-9f79-08bc7cc50e4c",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "---\n",
    "\n",
    "# Classification Metrics: Definitions and Use Cases\n",
    "\n",
    "In supervised learning, especially binary classification, evaluating model performance requires more than a single metric like accuracy. Depending on the **class imbalance**, **misclassification cost**, and **application domain**, one or more of the following metrics are used.\n",
    "\n",
    "Assume the binary classification confusion matrix is as follows:\n",
    "\n",
    "|                        | Predicted Positive | Predicted Negative |\n",
    "|------------------------|--------------------|--------------------|\n",
    "| **Actual Positive**    | True Positive (TP) | False Negative (FN)|\n",
    "| **Actual Negative**    | False Positive (FP)| True Negative (TN) |\n",
    "\n",
    "We now define each metric rigorously.\n",
    "\n",
    "---\n",
    "\n",
    "## 1. Accuracy\n",
    "\n",
    "**Definition**: The proportion of all correctly classified instances out of the total number of instances.\n",
    "\n",
    "\\[\n",
    "\\text{Accuracy} = \\frac{TP + TN}{TP + TN + FP + FN}\n",
    "\\]\n",
    "\n",
    "**When to use**: When classes are balanced and misclassification costs are symmetric.\n",
    "\n",
    "**Limitations**: Misleading in imbalanced datasets (e.g., high accuracy possible even when minority class is poorly predicted).\n",
    "\n",
    "---\n",
    "\n",
    "## 2. Recall (Sensitivity, True Positive Rate)\n",
    "\n",
    "**Definition**: The proportion of actual positive cases that are correctly identified.\n",
    "\n",
    "\\[\n",
    "\\text{Recall} = \\frac{TP}{TP + FN}\n",
    "\\]\n",
    "\n",
    "**When to use**: In scenarios where failing to detect positive cases is costly (e.g., medical diagnosis, fraud detection).\n",
    "\n",
    "---\n",
    "\n",
    "## 3. Precision\n",
    "\n",
    "**Definition**: The proportion of predicted positive cases that are truly positive.\n",
    "\n",
    "\\[\n",
    "\\text{Precision} = \\frac{TP}{TP + FP}\n",
    "\\]\n",
    "\n",
    "**When to use**: In scenarios where false positives are costly (e.g., spam filtering, legal applications).\n",
    "\n",
    "---\n",
    "\n",
    "## 4. F1 Score\n",
    "\n",
    "**Definition**: The harmonic mean of Precision and Recall, balancing the two.\n",
    "\n",
    "\\[\n",
    "\\text{F1} = 2 \\cdot \\frac{\\text{Precision} \\cdot \\text{Recall}}{\\text{Precision} + \\text{Recall}}\n",
    "\\]\n",
    "\n",
    "**When to use**: When a balance between false positives and false negatives is important, especially under class imbalance.\n",
    "\n",
    "---\n",
    "\n",
    "## 5. Specificity (True Negative Rate)\n",
    "\n",
    "**Definition**: The proportion of actual negatives that are correctly classified.\n",
    "\n",
    "\\[\n",
    "\\text{Specificity} = \\frac{TN}{TN + FP}\n",
    "\\]\n",
    "\n",
    "**When to use**: In domains where false positives must be minimized (e.g., judicial systems, quality control).\n",
    "\n",
    "---\n",
    "\n",
    "## 6. False Positive Rate (FPR)\n",
    "\n",
    "**Definition**: The proportion of actual negatives that are incorrectly classified as positives.\n",
    "\n",
    "\\[\n",
    "\\text{FPR} = \\frac{FP}{FP + TN}\n",
    "\\]\n",
    "\n",
    "**Note**: \\( \\text{FPR} = 1 - \\text{Specificity} \\)\n",
    "\n",
    "**When to use**: Often used in ROC analysis, where the trade-off between TPR and FPR is examined.\n",
    "\n",
    "---\n",
    "\n",
    "## 7. ROC AUC (Receiver Operating Characteristic - Area Under Curve)\n",
    "\n",
    "**Definition**: The area under the ROC curve, which plots True Positive Rate (TPR) vs. False Positive Rate (FPR) at various threshold levels.\n",
    "\n",
    "\\[\n",
    "\\text{ROC AUC} = \\int_{0}^{1} \\text{TPR}(x) \\, dx\n",
    "\\]\n",
    "\n",
    "**When to use**: To evaluate the modelâ€™s ability to discriminate between classes, especially under class imbalance.\n",
    "\n",
    "**Advantages**: Threshold-independent, robust to imbalance.\n",
    "\n",
    "---\n",
    "\n",
    "## 8. PR AUC (Precision-Recall Area Under Curve)\n",
    "\n",
    "**Definition**: The area under the Precision-Recall curve, which plots Precision vs. Recall across thresholds.\n",
    "\n",
    "\\[\n",
    "\\text{PR AUC} = \\int_{0}^{1} \\text{Precision}(\\text{Recall}) \\, d(\\text{Recall})\n",
    "\\]\n",
    "\n",
    "**When to use**: Preferable to ROC AUC in highly imbalanced datasets, where the minority class is of greater interest.\n",
    "\n",
    "---\n",
    "\n",
    "## 9. G-Mean (Geometric Mean)\n",
    "\n",
    "**Definition**: The geometric mean of Recall (TPR) and Specificity (TNR). Reflects balance in performance across both classes.\n",
    "\n",
    "\\[\n",
    "\\text{G-Mean} = \\sqrt{ \\text{Recall} \\cdot \\text{Specificity} } = \\sqrt{ \\frac{TP}{TP + FN} \\cdot \\frac{TN}{TN + FP} }\n",
    "\\]\n",
    "\n",
    "**When to use**: Particularly useful in imbalanced classification, where balanced performance on both classes is desired.\n",
    "\n",
    "---\n",
    "\n",
    "# Comparative Summary Table\n",
    "\n",
    "| **Metric**      | **Formula**                                                                 | **Sensitive to Imbalance** | **Threshold Dependent** | **Best Use Case**                                      |\n",
    "|-----------------|------------------------------------------------------------------------------|-----------------------------|--------------------------|---------------------------------------------------------|\n",
    "| Accuracy        | \\( \\frac{TP + TN}{TP + TN + FP + FN} \\)                                     | Yes                         | Yes                      | Balanced datasets                                       |\n",
    "| Recall (TPR)    | \\( \\frac{TP}{TP + FN} \\)                                                     | Yes                         | Yes                      | Prioritize detecting positives                          |\n",
    "| Precision       | \\( \\frac{TP}{TP + FP} \\)                                                     | Yes                         | Yes                      | Prioritize avoiding false positives                     |\n",
    "| F1 Score        | \\( 2 \\cdot \\frac{P \\cdot R}{P + R} \\)                                        | Yes                         | Yes                      | Balance between Precision and Recall                    |\n",
    "| Specificity     | \\( \\frac{TN}{TN + FP} \\)                                                     | Yes                         | Yes                      | Important to avoid false positives                      |\n",
    "| FPR             | \\( \\frac{FP}{FP + TN} \\)                                                     | Yes                         | Yes                      | ROC analysis and cost-sensitive evaluation              |\n",
    "| ROC AUC         | Area under ROC curve (TPR vs. FPR)                                           | No                          | No                       | Ranking ability of classifier under class imbalance     |\n",
    "| PR AUC          | Area under Precision-Recall curve                                            | Yes                         | No                       | Rare class detection (e.g., fraud, disease)             |\n",
    "| G-Mean          | \\( \\sqrt{ \\text{Recall} \\cdot \\text{Specificity} } \\)                        | Yes                         | Yes                      | Balanced performance across classes in imbalanced tasks |\n",
    "\n",
    "---\n",
    "\n",
    "Let me know if you'd like this in PDF or notebook format, or implemented in Python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29a77b31-04cb-4049-a834-d287561c389c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gpu6",
   "language": "python",
   "name": "gpu6"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
