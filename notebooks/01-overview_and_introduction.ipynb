{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "29de4e70-aae3-4e5e-bf70-653657b288a8",
   "metadata": {},
   "source": [
    "# 1. Introduction\n",
    "## Understanding Imbalanced Classification and Proposed Methodologies\n",
    "\n",
    "In real-world classification problems, class imbalance is a frequent challenge where one class (typically the \"positive\" or event class) is significantly underrepresented compared to the other. This imbalance often leads to biased models that favor the majority class, compromising the ability to detect minority class instances effectively. Data-level approaches, such as oversampling and undersampling, address this issue by modifying the training data distribution without changing the model. Techniques like SMOTE (Synthetic Minority Over-sampling Technique) synthetically generate new samples to boost the minority class, while Edited Nearest Neighbors (ENN) helps remove noisy or ambiguous examples. Hybrid solutions, such as SMOTEENN, combine oversampling with intelligent undersampling, aiming to enhance class balance while preserving data quality. These strategies are particularly useful in improving recall, precision, and overall performance on imbalanced datasets before feeding them into machine learning models.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "The primary difficulties in learning from imbalanced data include:\n",
    "\n",
    "- **Biased predictions** toward the majority class\n",
    "- **Misleading accuracy metrics** that overlook minority class performance\n",
    "- **Poor generalization** to unseen data, particularly for minority instances\n",
    "- **Increased false negatives**, where the model fails to identify critical minority class cases\n",
    "\n",
    "Since accuracy is not a reliable metric in imbalanced settings, alternative evaluation metrics such as\n",
    "**precision**, **recall**, **F1-score**, and the **area under the precision-recall curve (PR AUC)** are preferred.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5c5c2c2-06d2-414e-a6c6-65ff28b9ab8b",
   "metadata": {},
   "source": [
    "# 2.Problem and Methodologies\n",
    "\n",
    "To address these challenges, the literature suggests a taxonomy of solutions including:\n",
    "### 1. Data-Level Methods\n",
    "These techniques adjust the training data distribution to better represent all classes. Common approaches include:\n",
    "\n",
    "- **Oversampling** the minority class (e.g., **SMOTE**, **Borderline-SMOTE**, **ADASYN**)\n",
    "- **Undersampling** the majority class (e.g., random or **cluster-based undersampling**)\n",
    "- **Combined resampling** strategies that use both oversampling and undersampling\n",
    "\n",
    "Data-level methods are advantageous as they are model-agnostic and can be used independently of the classifier.\n",
    "\n",
    "### 2. Hybrid Methods\n",
    "Hybrid techniques combine data-level preprocessing with algorithm-level improvements to harness the strengths of both. For example:\n",
    "\n",
    "- **SMOTE + Cost-Sensitive Learning**\n",
    "- **SMOTEBoost** and **RUSBoost**, which integrate data balancing with boosting algorithms\n",
    "- **Ensemble learning** combined with hybrid sampling techniques\n",
    "\n",
    "These approaches are particularly effective in improving model robustness, reducing overfitting, and retaining valuable information.\n",
    "\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b3e7ae2-7c8c-4368-b63b-089e8a06c96e",
   "metadata": {},
   "source": [
    "## Basic SMOTE Explained"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "566ed15e-1358-4cc9-8355-0ec10cdc0ed0",
   "metadata": {},
   "source": [
    "![SMOTE Technique Explained](smote.jpeg)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "178f09a7-23af-4be1-a86e-d8d454c0c4c3",
   "metadata": {},
   "source": [
    "# 2. Problems and Methodologies "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b737220-1f54-417d-9ff0-2893b9d51df4",
   "metadata": {},
   "source": [
    "\n",
    "### ðŸ”¹ 1. SMOTE (Synthetic Minority Oversampling Technique)\n",
    "\n",
    "**Concept:**  \n",
    "SMOTE is a powerful oversampling technique that generates synthetic samples for the minority class rather than duplicating existing ones. It interpolates between a sample and its nearest neighbors in the feature space.\n",
    "\n",
    "**How it works:**\n",
    "1. For each minority class sample, SMOTE identifies *k nearest neighbors* (typically `k=5`).\n",
    "2. A neighbor is selected randomly, and a synthetic sample is created on the line segment joining the original sample and the neighbor.\n",
    "3. This synthetic point is added to the dataset.\n",
    "\n",
    "**Why it's useful:**  \n",
    "It increases diversity among the minority class samples and helps the classifier better learn the minority class boundaries. However, by itself, SMOTE may introduce overlapping or noisy samples near the majority class.\n",
    "\n",
    "---\n",
    "\n",
    "### ðŸ”¹ 2. SVM-SMOTE (Support Vector Machineâ€“Based SMOTE)\n",
    "\n",
    "**Concept:**  \n",
    "SVM-SMOTE is an enhancement over classical SMOTE. It uses a trained Support Vector Machine (SVM) classifier to **identify the support vectors** â€” data points near the decision boundary â€” and focuses sample generation near these critical regions.\n",
    "\n",
    "**How it works:**\n",
    "1. An SVM is trained to find the boundary between classes.\n",
    "2. Support vectors from the minority class are selected.\n",
    "3. Synthetic samples are generated near these support vectors.\n",
    "\n",
    "**Why it's useful:**  \n",
    "By concentrating synthetic data near the decision boundary, SVM-SMOTE improves the **discriminative power** of the model and reduces the risk of overgeneralization from too many easy examples.\n",
    "\n",
    "---\n",
    "\n",
    "### ðŸ”¹ 3. Distance-Weighted Edited Nearest Neighbors (ENN)\n",
    "\n",
    "**Concept:**  \n",
    "ENN is a data cleaning method that removes noisy or ambiguous samples, particularly from the majority class, by evaluating how well a point agrees with its neighbors.\n",
    "\n",
    "**Enhancement with Distance Weights:**  \n",
    "I implemented **distance-weighted ENN** with various strategies for weighting the influence of neighbors:\n",
    "- **Inverse distance:** closer neighbors have higher weight.\n",
    "- **Gaussian decay:** exponential decay of weight with squared distance.\n",
    "- **Exponential decay:** sharper drop-off than Gaussian.\n",
    "- **Rank-based:** closer ranks get more importance.\n",
    "- **Adaptive power decay:** tunable control over influence using a beta parameter.\n",
    "\n",
    "**Why it's useful:**  \n",
    "These weighting schemes allow more nuanced filtering. For example, if a majority class sample is surrounded by minority class neighbors but farther away, its influence can be discounted rather than completely discarded.\n",
    "\n",
    "---\n",
    "\n",
    "### ðŸ”¹ 4. SMOTEENN with Weighted Voting\n",
    "\n",
    "**Concept:**  \n",
    "This is a **hybrid resampling pipeline** combining oversampling (SMOTE) and undersampling (ENN).\n",
    "\n",
    "**Steps:**\n",
    "1. Use SMOTE to generate new minority samples.\n",
    "2. Apply distance-weighted ENN to remove noisy or misclassified samples, mainly from the majority class.\n",
    "\n",
    "**Why it's useful:**  \n",
    "This two-step process balances the dataset while **retaining only clean and informative samples**, improving the decision boundary and reducing overfitting caused by noisy or borderline examples.\n",
    "\n",
    "---\n",
    "\n",
    "### ðŸ”¹ 5. SMOTified-GAN (SMOTE + GAN)\n",
    "\n",
    "**Concept:**  \n",
    "To go beyond traditional synthetic sample generation, I developed a **SMOTified-GAN**, a type of Generative Adversarial Network (GAN) that creates realistic and diverse samples for the minority class by learning its data distribution.\n",
    "\n",
    "**How it works:**\n",
    "1. A generator network learns to create synthetic data conditioned on the minority class.\n",
    "2. A discriminator network tries to distinguish between real and fake data.\n",
    "3. The generator improves over time to fool the discriminator, producing high-quality samples.\n",
    "\n",
    "**Why it's useful:**  \n",
    "Unlike SMOTE, which relies on linear interpolation, **SMOTified-GAN can learn complex nonlinear data distributions**, resulting in synthetic samples that better resemble real data. This boosts the modelâ€™s ability to generalize to unseen minority class examples.\n",
    "\n",
    "---\n",
    "\n",
    "### âœ… Summary of Advantages\n",
    "\n",
    "| Method             | Type         | Key Strengths |\n",
    "|--------------------|--------------|----------------|\n",
    "| SMOTE              | Oversampling | Simple and effective; increases minority presence |\n",
    "| SVM-SMOTE          | Oversampling | Focuses on decision boundary; better for hard examples |\n",
    "| Distance-weighted ENN | Undersampling | Flexible cleaning with smart neighbor voting |\n",
    "| SMOTEENN           | Hybrid       | Balances dataset + denoises it |\n",
    "| SMOTified-GAN      | Oversampling | Realistic and diverse synthetic samples |\n",
    "\n",
    "---\n",
    "\n",
    "By integrating these **data-level and hybrid solutions**, I tackled the class imbalance problem from multiple perspectives â€” improving not just quantity but also the **quality and diversity** of training data. This led to **more robust, generalizable, and fair classifiers** across my experiments.\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fbd0030-4726-4081-b695-ff59ccaa939f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dff00ddf-2076-46a0-abca-5d3ce2e6da94",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64daf713-cbce-4484-b47b-f084c101ae8b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b53f9e4f-2485-48e7-80a1-15159fea99aa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa96e01b-439b-42ee-b930-1cb7fcb2440a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6605bf85-6e47-469b-a1c3-98d73f60cc3b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f650888-9ddd-4994-b38e-bdbd09f51038",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c406fc0-39ea-4446-88e7-847e2c6b6fae",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71d94139-41f5-495b-a4aa-94d2d9e0532b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gpu6",
   "language": "python",
   "name": "gpu6"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
