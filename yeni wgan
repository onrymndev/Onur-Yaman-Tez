import numpy as np
import torch
from torch import nn, autograd
from torch.utils.data import TensorDataset, DataLoader
from tqdm.auto import tqdm
from lightgbm import LGBMClassifier
from sklearn.model_selection import train_test_split

# ============== Blocks ==============
def get_generator_block(input_dim, output_dim):
    return nn.Sequential(
        nn.Linear(input_dim, output_dim),
        nn.BatchNorm1d(output_dim),
        nn.ReLU(inplace=True),
    )

class ConditionalGenerator(nn.Module):
    def __init__(self, z_dim, label_dim, im_dim, hidden_dim=128):
        super().__init__()
        self.gen = nn.Sequential(
            get_generator_block(z_dim + label_dim, hidden_dim),
            get_generator_block(hidden_dim, hidden_dim * 2),
            get_generator_block(hidden_dim * 2, hidden_dim * 4),
            get_generator_block(hidden_dim * 4, hidden_dim * 8),
            nn.Linear(hidden_dim * 8, im_dim),
        )

    def forward(self, noise, labels):
        x = torch.cat((noise, labels), dim=1)
        return self.gen(x)

class ConditionalCritic(nn.Module):
    def __init__(self, im_dim, label_dim, hidden_dim=128):
        super().__init__()
        self.disc = nn.Sequential(
            nn.Linear(im_dim + label_dim, hidden_dim * 4),
            nn.LeakyReLU(0.2),
            nn.Linear(hidden_dim * 4, hidden_dim * 2),
            nn.LeakyReLU(0.2),
            nn.Linear(hidden_dim * 2, hidden_dim),
            nn.Linear(hidden_dim, 1)
        )

    def forward(self, x, labels):
        x = torch.cat((x, labels), dim=1)
        return self.disc(x)

# A tiny classifier C for C-Reg (binary)
class SmallClassifier(nn.Module):
    def __init__(self, im_dim, hidden=128):
        super().__init__()
        self.net = nn.Sequential(
            nn.Linear(im_dim, hidden),
            nn.ReLU(inplace=True),
            nn.Linear(hidden, hidden//2),
            nn.ReLU(inplace=True),
            nn.Linear(hidden//2, 1)
        )
    def forward(self, x):
        return self.net(x)  # logits

# ============== Utilities ==============
def shuffle_in_unison(a, b):
    p = np.random.permutation(len(a))
    return a[p], b[p]

# --------- Novel: Feature-Weighted Gradient Penalty (FW-GP) ----------
def gradient_penalty_weighted(critic, real, fake, labels, device, feat_weights_t, lambda_gp=10.0):
    """
    Enforce ||grad_x D||_{w,2} ≈ 1 where weighted norm is sqrt(sum_i w_i * g_i^2)
    """
    batch_size = real.size(0)
    alpha = torch.rand(batch_size, 1, device=device).expand_as(real)
    interpolated = (alpha * real + (1 - alpha) * fake).requires_grad_(True)
    critic_interpolated = critic(interpolated, labels)

    grad = autograd.grad(
        outputs=critic_interpolated,
        inputs=interpolated,
        grad_outputs=torch.ones_like(critic_interpolated),
        create_graph=True,
        retain_graph=True,
        only_inputs=True
    )[0]  # (B, D)

    # weighted L2 norm per sample: sqrt( sum_i w_i * g_i^2 )
    # feat_weights_t shape: (D,)
    g2 = grad.pow(2)
    weighted_g2 = g2 * feat_weights_t  # broadcast (B,D) * (D,)
    norm_w = torch.sqrt(weighted_g2.sum(dim=1) + 1e-12)  # (B,)
    gp = ((norm_w - 1.0) ** 2).mean() * lambda_gp
    return gp

# ============== Main function ==============
def wgan_gp_gain_weighted_novel(
    X_train,
    y_train,
    n_epochs=200,
    batch_size=128,
    critic_iterations=5,
    lr=1e-4,
    lambda_gp=10.0,
    alpha_c_reg=0.5,   # weight of classifier guidance in G-loss
    noise_keep_var=True # normalize noise weights to keep avg var ~ 1
):
    """
    Novel WGAN-GP for oversampling:
      - Feature-weighted latent noise (via LightGBM gain importances)
      - Feature-Weighted GP (FW-GP): enforces Lipschitz stronger on important features
      - Classifier-in-the-loop regularization (C-Reg): pushes G to produce samples C thinks are minority
    """
    device = 'cuda' if torch.cuda.is_available() else 'cpu'
    X_train = np.asarray(X_train)
    y_train = np.asarray(y_train).ravel().astype(int)
    input_dim = X_train.shape[1]

    # ---- 1) Importances from LightGBM (gain) ----
    lgb = LGBMClassifier(n_estimators=300)
    lgb.fit(X_train, y_train)
    gains = lgb.booster_.feature_importance(importance_type="gain")
    gains = np.array(gains, dtype=np.float64)
    # avoid zero division
    if gains.sum() == 0:
        gains = np.ones_like(gains, dtype=np.float64)
    w = gains / gains.sum()  # sum=1
    # torch weights (float32)
    w_t = torch.tensor(w, dtype=torch.float32, device=device)

    # ---- 2) Latent noise weights (variance along important features)
    # scale by sqrt(w). Optionally renormalize so average variance ≈ 1
    z_scale = np.sqrt(w).astype(np.float32)
    if noise_keep_var:
        # Make mean(z_scale^2) == 1  => scale so average feature variance unchanged
        z_scale = z_scale / np.sqrt((z_scale**2).mean() + 1e-12)
    z_scale_t = torch.tensor(z_scale, dtype=torch.float32, device=device)

    # ---- 3) Split minority and compute how many to generate (balance)
    X_min = X_train[y_train == 1]
    X_maj = X_train[y_train == 0]
    n_min, n_maj = len(X_min), len(X_maj)
    n_generate = max(n_maj - n_min, 0)

    # ---- 4) Dataloaders on minority only (as in your setup)
    tensor_x = torch.tensor(X_min, dtype=torch.float32)
    tensor_y = torch.ones((len(X_min),), dtype=torch.float32)
    dataset = TensorDataset(tensor_x, tensor_y)
    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True, drop_last=len(X_min) >= batch_size)

    # ---- 5) Models ----
    label_dim = 1
    gen   = ConditionalGenerator(z_dim=input_dim, label_dim=label_dim, im_dim=input_dim).to(device)
    critic= ConditionalCritic(im_dim=input_dim, label_dim=label_dim).to(device)

    gen_opt    = torch.optim.Adam(gen.parameters(),    lr=lr, betas=(0.0, 0.9))
    critic_opt = torch.optim.Adam(critic.parameters(), lr=lr, betas=(0.0, 0.9))

    # ---- 6) Pretrain a tiny classifier C for C-Reg ----
    Xtr, Xva, ytr, yva = train_test_split(X_train, y_train, test_size=0.2, stratify=y_train, random_state=42)
    C = SmallClassifier(im_dim=input_dim).to(device)
    C_opt = torch.optim.Adam(C.parameters(), lr=1e-3)
    bce = nn.BCEWithLogitsLoss()
    Xtr_t = torch.tensor(Xtr, dtype=torch.float32, device=device)
    ytr_t = torch.tensor(ytr.astype(np.float32), dtype=torch.float32, device=device).unsqueeze(1)
    # quick few epochs
    C.train()
    for _ in range(5):
        idx = torch.randperm(Xtr_t.size(0), device=device)
        for i in range(0, Xtr_t.size(0), 256):
            j = idx[i:i+256]
            xb, yb = Xtr_t[j], ytr_t[j]
            C_opt.zero_grad()
            loss_c = bce(C(xb), yb)
            loss_c.backward()
            C_opt.step()
    C.eval()  # freeze during GAN

    # ---- 7) Train WGAN-GP with FW-GP + C-Reg ----
    for epoch in range(n_epochs):
        for real, _ in tqdm(dataloader, leave=False):
            real = real.to(device)
            real_labels = torch.ones((real.size(0), 1), device=device)

            # --- Critic updates ---
            for _ in range(critic_iterations):
                z = torch.randn(real.size(0), input_dim, device=device) * z_scale_t  # feature-weighted noise
                noise_labels = torch.ones((real.size(0), 1), device=device)
                fake = gen(z, noise_labels)

                critic_real = critic(real, real_labels).mean()
                critic_fake = critic(fake.detach(), noise_labels).mean()
                gp = gradient_penalty_weighted(critic, real, fake.detach(), real_labels, device, w_t, lambda_gp=lambda_gp)

                critic_loss = -(critic_real - critic_fake) + gp
                critic_opt.zero_grad()
                critic_loss.backward()
                critic_opt.step()

            # --- Generator update (standard WGAN loss + C-Reg) ---
            z = torch.randn(real.size(0), input_dim, device=device) * z_scale_t
            noise_labels = torch.ones((real.size(0), 1), device=device)
            gen_fake = gen(z, noise_labels)

            # WGAN loss
            g_wgan = -critic(gen_fake, noise_labels).mean()

            # Classifier guidance: make C think it's minority (target 1)
            with torch.no_grad():
                pass
            g_creg = bce(C(gen_fake), torch.ones_like(gen_fake[:, :1]))  # use first logit target=1

            gen_loss = g_wgan + alpha_c_reg * g_creg

            gen_opt.zero_grad()
            gen_loss.backward()
            gen_opt.step()

        print(f"Epoch {epoch:03d} | G: {gen_loss.item():.4f} | D: {critic_loss.item():.4f}")

    # ---- 8) Generate exactly enough to balance ----
    with torch.no_grad():
        if n_generate > 0:
            z = torch.randn(n_generate, input_dim, device=device) * z_scale_t
            noise_labels = torch.ones((n_generate, 1), device=device)
            generated = gen(z, noise_labels).cpu().numpy()
            y_gen = np.ones((n_generate,), dtype=int)
            X_out = np.concatenate([X_train, generated], axis=0)
            y_out = np.concatenate([y_train, y_gen], axis=0)
        else:
            X_out, y_out = X_train.copy(), y_train.copy()

    # shuffle
    X_out, y_out = shuffle_in_unison(X_out, y_out)
    return X_out, y_out